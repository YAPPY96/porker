import rlcard
from rlcard.agents import DQNAgent
import torch
import os
import glob
import numpy as np
from PIL import Image

# --- GUIã‚¨ãƒ©ãƒ¼å›é¿ç”¨è¨­å®š ---
import matplotlib
matplotlib.use('Agg') # ç”»é¢è¡¨ç¤ºã›ãšæç”»ã™ã‚‹ãƒ¢ãƒ¼ãƒ‰
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from blackjack_utils import get_score

# ---------------------------------------------------------
# è¨­å®š
# ---------------------------------------------------------
SAVE_DIR = 'experiments/blackjack_custom_reward'
OUTPUT_DIR = 'replays' # GIFã®ä¿å­˜å…ˆ
GAMES_TO_RECORD = 1 # å„æ€§æ ¼ã«ã¤ãä½•ã‚²ãƒ¼ãƒ éŒ²ç”»ã™ã‚‹ã‹

if not os.path.exists(OUTPUT_DIR):
    os.makedirs(OUTPUT_DIR)

# ---------------------------------------------------------
# æç”»ç”¨ãƒ˜ãƒ«ãƒ‘ãƒ¼é–¢æ•°
# ---------------------------------------------------------
def draw_card(ax, x, y, card_str):
    """ã‚«ãƒ¼ãƒ‰1æšã‚’æç”»ã™ã‚‹é–¢æ•°"""
    # ã‚«ãƒ¼ãƒ‰ã®æ 
    rect = patches.Rectangle((x, y), 0.8, 1.2, linewidth=1, edgecolor='black', facecolor='white', zorder=2)
    ax.add_patch(rect)
    
    # ã‚¹ãƒ¼ãƒˆã¨æ•°å­—ã®å¤‰æ›
    if card_str == 'BACK':
        # è£é¢
        pattern = patches.Rectangle((x+0.1, y+0.1), 0.6, 1.0, facecolor='firebrick', zorder=3)
        ax.add_patch(pattern)
        return

    suit_map = {'S': 'â™ ', 'H': 'â™¥', 'D': 'â™¦', 'C': 'â™£'}
    color_map = {'S': 'black', 'H': 'red', 'D': 'red', 'C': 'black'}
    
    suit_char = card_str[0]
    rank_char = card_str[1:]
    
    suit = suit_map.get(suit_char, suit_char)
    color = color_map.get(suit_char, 'black')
    
    # ä¸­å¤®ã®æ–‡å­—
    ax.text(x + 0.4, y + 0.6, f"{rank_char}\n{suit}", fontsize=15, 
            ha='center', va='center', color=color, zorder=4)
    # å·¦ä¸Šã®æ–‡å­—
    ax.text(x + 0.1, y + 1.0, rank_char, fontsize=8, color=color, zorder=4)

def create_frame(player_hand, dealer_hand, action_text, result_text, score, personality):
    """ç¾åœ¨ã®ç›¤é¢ã‚’ç”»åƒã¨ã—ã¦ç”Ÿæˆã™ã‚‹"""
    fig, ax = plt.subplots(figsize=(6, 6))
    ax.set_facecolor('#006400') # ã‚«ã‚¸ãƒã£ã½ã„ç·‘è‰²ã®èƒŒæ™¯
    ax.set_xlim(0, 6)
    ax.set_ylim(0, 6)
    ax.axis('off') # è»¸ã‚’æ¶ˆã™

    # ã‚¿ã‚¤ãƒˆãƒ«
    ax.text(3, 5.5, f"Agent: {personality}", fontsize=16, ha='center', color='white', fontweight='bold')
    
    # --- ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã®æç”» (ä¸Šæ®µ) ---
    ax.text(0.5, 4.5, "Dealer", fontsize=12, color='white')
    for i, card in enumerate(dealer_hand):
        draw_card(ax, 1.5 + i * 1.0, 3.5, card)
    
    # --- ãƒ—ãƒ¬ã‚¤ãƒ¤ãƒ¼ã®æç”» (ä¸‹æ®µ) ---
    ax.text(0.5, 1.5, f"Player\nScore: {score}", fontsize=12, color='white')
    for i, card in enumerate(player_hand):
        draw_card(ax, 1.5 + i * 1.0, 0.5, card)

    # --- ã‚¢ã‚¯ã‚·ãƒ§ãƒ³/çµæœã®è¡¨ç¤º ---
    if result_text:
        # çµæœãŒå‡ºã¦ã„ã‚‹å ´åˆ
        box_color = 'gold' if "WIN" in result_text else 'gray'
        ax.text(3, 2.5, result_text, fontsize=24, color='blue', ha='center', 
                bbox=dict(facecolor=box_color, alpha=0.8))
    elif action_text:
        # è¡Œå‹•ä¸­ã®å ´åˆ
        ax.text(3, 2.5, f"Action: {action_text}", fontsize=18, color='yellow', ha='center', fontweight='bold')

    # ãƒ¡ãƒ¢ãƒªä¸Šã®ç”»åƒãƒ‡ãƒ¼ã‚¿ã«å¤‰æ›
    fig.canvas.draw()
    
    # æ–°ã—ã„æ›¸ãæ–¹: buffer_rgba() ã‚’ä½¿ã£ã¦é…åˆ—ã¨ã—ã¦å–å¾—
    # è‡ªå‹•çš„ã« (é«˜ã•, å¹…, 4) ã® RGBA é…åˆ—ã«ãªã‚Šã¾ã™
    image_array = np.asarray(fig.canvas.buffer_rgba())
    
    # PILã¯ RGBA é…åˆ—ã‚’ãã®ã¾ã¾ç”»åƒã«å¤‰æ›ã§ãã¾ã™
    plt.close(fig)
    return Image.fromarray(image_array)

# ---------------------------------------------------------
# ãƒ¡ã‚¤ãƒ³å‡¦ç†
# ---------------------------------------------------------
# ãƒ¢ãƒ‡ãƒ«ã‚’æ¢ã™
model_files = glob.glob(os.path.join(SAVE_DIR, 'model_*.pth'))
model_files.sort()

env = rlcard.make('blackjack')
agent = DQNAgent(num_actions=env.num_actions, state_shape=env.state_shape[0], mlp_layers=[128, 128], device=torch.device("cpu"))

print(f"Generating replays for {len(model_files)} models...\n")

for model_path in model_files:
    personality = os.path.basename(model_path).replace('model_', '').replace('.pth', '')
    print(f"Creating replay for: {personality}")

    # ãƒ¢ãƒ‡ãƒ«ãƒ­ãƒ¼ãƒ‰
    agent.q_estimator.qnet.load_state_dict(torch.load(model_path))
    
    frames = []
    
    for _ in range(GAMES_TO_RECORD):
        state, player_id = env.reset()
        
        # ã‚²ãƒ¼ãƒ é–‹å§‹æ™‚ã®çŠ¶æ…‹
        raw_obs = state['raw_obs']
        p_hand = raw_obs['player0 hand']
        d_hand = raw_obs['dealer hand'] # ã“ã“ã§ã¯1æšã—ã‹è¦‹ãˆã¦ãªã„æƒ³å®š
        
        # ãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã®æ‰‹æœ­è¡¨ç¤ºãƒ­ã‚¸ãƒƒã‚¯ï¼ˆæœ€åˆã¯1æšï¼‹è£é¢ï¼‰
        display_d_hand = [d_hand[0], 'BACK'] 
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ 1: é…ã‚‰ã‚ŒãŸç›´å¾Œ
        frames.append(create_frame(p_hand, display_d_hand, "Thinking...", None, get_score(p_hand), personality))
        
        done = False
        while not env.is_over():
            action, _ = agent.eval_step(state)
            act_str = "Hit" if action == 0 else "Stand"
            
            # ãƒ•ãƒ¬ãƒ¼ãƒ 2: æ±ºæ–­
            frames.append(create_frame(p_hand, display_d_hand, act_str, None, get_score(p_hand), personality))
            
            state, next_player_id = env.step(action, player_id)
            
            # çŠ¶æ…‹æ›´æ–°
            raw_obs = state['raw_obs']
            p_hand = raw_obs['player0 hand']
            
            # Hitã—ãŸå ´åˆã€ã‚«ãƒ¼ãƒ‰ãŒå¢—ãˆãŸçŠ¶æ…‹ã‚’è¡¨ç¤º
            if action == 0 and not env.is_over():
                frames.append(create_frame(p_hand, display_d_hand, "Hit!", None, get_score(p_hand), personality))

        # --- çµæœè¡¨ç¤º ---
        raw_obs = state['raw_obs']
        p_hand = raw_obs['player0 hand']
        d_hand = raw_obs['dealer hand'] # å…¨ã¦å…¬é–‹
        
        payoffs = env.get_payoffs()
        score = payoffs[player_id]
        
        res_text = "WIN ğŸ†" if score > 0 else ("LOSE ğŸ’€" if score < 0 else "DRAW ğŸ¤")
        
        # ãƒ•ãƒ¬ãƒ¼ãƒ 3: æœ€çµ‚çµæœï¼ˆãƒ‡ã‚£ãƒ¼ãƒ©ãƒ¼ã®æ‰‹æœ­ã‚ªãƒ¼ãƒ—ãƒ³ï¼‰
        # æœ€å¾Œã®ä½™éŸ»ã®ãŸã‚ã«åŒã˜ãƒ•ãƒ¬ãƒ¼ãƒ ã‚’æ•°æšè¿½åŠ 
        end_frame = create_frame(p_hand, d_hand, None, res_text, get_score(p_hand), personality)
        for _ in range(5):
            frames.append(end_frame)

    # GIFä¿å­˜
    gif_path = os.path.join(OUTPUT_DIR, f'replay_{personality}.gif')
    # duration=800 ã¯ 0.8ç§’ã”ã¨ã«ã‚³ãƒé€ã‚Š
    frames[0].save(gif_path, save_all=True, append_images=frames[1:], optimize=False, duration=800, loop=0)
    print(f"  -> Saved: {gif_path}")

print(f"\nAll replays saved in '{OUTPUT_DIR}' folder! ğŸ¥")